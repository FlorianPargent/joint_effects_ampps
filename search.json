[
  {
    "objectID": "supplement_1.html",
    "href": "supplement_1.html",
    "title": "Supplement 1: Estimate expected potential outcomes in a hypothetical RCT",
    "section": "",
    "text": "In our manuscript, we introduce the following hypothetical RCT in the section Different Estimands in a Longitudinal Study:\nConsider a smartphone study in which participants are randomly assigned to use their smartphone from 8 PM to 10 PM (\\(t = 1\\)) or not. No intervention occurs afterward, which means that from 10 PM to midnight (\\(t = 2\\)), participants can decide for themselves whether to use their smartphone and this usage is passively recorded. We set \\(a_t=1\\) if there was any smartphone usage during time frame \\(t\\), while \\(a_t=0\\) means that the smartphone was not used at all during \\(t\\). At 10 PM, participants’ rumination (\\(L\\)) is also measured, as it is hypothesized to be another mediator between smartphone usage and sleep quality. Sleep quality (\\(Y\\)), the final outcome of the study, is measured the following morning. Figure 1 presents a DAG corresponding to this RCT.\nHere in Supplement 1, we will demonstrate in R how we can simulate and estimate expected potential outcomes from this hypothetical RCT."
  },
  {
    "objectID": "supplement_1.html#load-packages",
    "href": "supplement_1.html#load-packages",
    "title": "Supplement 1: Estimate expected potential outcomes in a hypothetical RCT",
    "section": "Load packages",
    "text": "Load packages\nFor our demonstration, we need the following packages. The package versions we used are recorded in the renv.lock file in our code repository.\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(brms)\nlibrary(cmdstanr)\nlibrary(tidybayes)"
  },
  {
    "objectID": "supplement_1.html#specify-the-data-generating-process",
    "href": "supplement_1.html#specify-the-data-generating-process",
    "title": "Supplement 1: Estimate expected potential outcomes in a hypothetical RCT",
    "section": "Specify the data generating process",
    "text": "Specify the data generating process\n\nDraw the DAG\nThe following DAG represents the assumed causal relationships for the hypothetical RCT discussed in the manuscript.\n\n\nCode\ndag &lt;- dagitty('dag{\n  A1 -&gt; L &lt;- U\n  A1 -&gt; A2 &lt;- L\n  A1 -&gt; Y &lt;- A2\n  L -&gt; Y &lt;- U\n  A1[pos=\"0,2\"]\n  A2[pos=\"1,2\"]\n  Y[pos=\"1.5,1.5\"]\n  L[pos=\"0.5,1.5\"]\n  U[pos=\"1,1\"]\n  }')\nplot(dag)\n\n\n\n\n\n\n\n\nFigure 1: DAG that corresponds to an RCT with repeated measurements. \\(U\\) denotes unmeasured confounding, while \\(L\\) and \\(A_2\\) are measured mediators of the causal effect of \\(A_1\\) on the end of study outcome \\(Y\\). Since \\(A_1\\) is randomized, there are no arrows pointing into it.\n\n\n\n\n\n\n\nSpecify the functional relationships\nThe DAG only specifies the causal relationships, e.g., \\(L\\) is a function of \\(A_1\\) and \\(U\\). However, this information does not fully specify the data generating process. To simulate data, we must specify the exact functions that produce all variables. We use gaussian linear models for continuous variables (\\(L\\) and \\(Y\\)) and a probit model for the binary variable \\(A_2\\), while \\(A_1\\) is fully randomized.\nIn our example, \\(U\\) affects \\(A_2\\) only through the measured variable \\(L\\). While this assumption is unlikely to hold in real-world scenarios, it is required for joint-effect estimation. We discuss situations in which this assumption may be plausible in psychological research—induced by covariate-driven treatment assignment—in the latter half of the manuscript.\n\nb_L_A1 &lt;- 1\nb_L_U &lt;- 1\nb_A2_A1 &lt;- -3\nb_A2_L &lt;- 0.5\nb_A2_U &lt;- 0\nb_Y_A1 &lt;- -0.1\nb_Y_A2 &lt;- -3\nb_Y_L &lt;- -1\nb_Y_A1A2 &lt;- -0.5\nb_Y_U &lt;- -2\n\n\nf_U &lt;- function(){\n  rnorm(n)}\nf_A1 &lt;- function(){\n  rbinom(n, size = 1, prob = 0.5)}\nf_L &lt;- function(A1, U){\n  rnorm(n, mean = b_L_A1 * A1 + b_L_U * U)}\nf_A2 &lt;- function(A1, L){\n  rbinom(n, size = 1, prob = pnorm(b_A2_A1 * A1 + b_A2_L* L + b_A2_U * U))}\nf_Y &lt;- function(A1, A2, L, U){\n  rnorm(n, mean = 10 + b_Y_A1 * A1 + b_Y_A2 * A2 + b_Y_A1A2 * A1 * A2 + b_Y_L * L +\n      b_Y_U * U, sd = 0.1)}"
  },
  {
    "objectID": "supplement_1.html#determine-the-true-expected-potential-outcomes",
    "href": "supplement_1.html#determine-the-true-expected-potential-outcomes",
    "title": "Supplement 1: Estimate expected potential outcomes in a hypothetical RCT",
    "section": "Determine the true expected potential outcomes",
    "text": "Determine the true expected potential outcomes\nThe manuscript focuses on joint effects, which in our example are a difference of 4 different expected potential outcomes. We will now determine the true value of all four potential outcomes using simulation. We always set \\(A_1\\) and \\(A_2\\) to the values chosen by the respective hypothetical intervention but produce the remaining variables according to the assumptions of our data generating process.\n\n“Never use”: \\(E(Y^{a_1=0, a_2=0})\\)\nWe call the potential outcome \\(Y^{a_1=0, a_2=0}\\) “never use” because it represents the sleep quality of a person that (by a hypothetical intervention) was forced not to use their smartphone on both time points.\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_00: Y^{a_1=0, a_2=0}\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(0, n) # intervention\nY_00 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_00)\n(E_Y_00 &lt;- mean(Y_00))\n\n[1] 9.998539\n\n\n\n\n“Always use”: \\(E(Y^{a_1=1, a_2=1})\\)\nWe call the potential outcome \\(Y^{a_1=1, a_2=1}\\) “always use” because it represents the sleep quality of a person that (by a hypothetical intervention) was forced to use their smartphone on both time points.\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_11: Y^{a_1=1, a_2=1}\nU &lt;- f_U()\nA1 &lt;- rep(1, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(1, n) # intervention\nY_11 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_11)\n(E_Y_11 &lt;- mean(Y_11))\n\n[1] 5.398539\n\n\n\n\n“Early use”: \\(E(Y^{a_1=1, a_2=0})\\)\nWe call the potential outcome \\(Y^{a_1=1, a_2=0}\\) “early use” because it represents the sleep quality of a person that (by a hypothetical intervention) was forced to use their smartphone on the first time point but not on the second time point.\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_10: Y^{a_1=1, a_2=0}\nU &lt;- f_U()\nA1 &lt;- rep(1, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(0, n) # intervention\nY_10 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_11)\n(E_Y_10 &lt;- mean(Y_10))\n\n[1] 8.898539\n\n\n\n\n“Late use”: \\(E(Y^{a_1=0, a_2=1})\\)\nWe call the potential outcome \\(Y^{a_1=0, a_2=1}\\) “late use” because it represents the sleep quality of a person that (by a hypothetical intervention) was forced not to use their smartphone on the first time point but on the second time point.\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_01: Y^{a_1=0, a_2=1}\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(1, n) # intervention\nY_01 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_01)\n(E_Y_01 &lt;- mean(Y_01))\n\n[1] 6.998539"
  },
  {
    "objectID": "supplement_1.html#simulate-a-sample-dataset",
    "href": "supplement_1.html#simulate-a-sample-dataset",
    "title": "Supplement 1: Estimate expected potential outcomes in a hypothetical RCT",
    "section": "Simulate a sample dataset",
    "text": "Simulate a sample dataset\nWe now simulate a single dataset from our assumed data generating process. Remember, this process assumes that the smartphone usage on \\(A_1\\) is fully randomized, but participants can decide for themselves, whether they want to use their smartphone on \\(A_2\\).\n\nn &lt;- 2000\nset.seed(42)\n\nU &lt;- f_U()\nA1 &lt;- f_A1()\nL &lt;- f_L(A1, U)\nA2 &lt;- f_A2(A1, L)\nY &lt;- f_Y(A1, A2, L, U)\n\ndat &lt;- data.frame(A1, A2, L, Y)"
  },
  {
    "objectID": "supplement_1.html#fit-a-statistical-model-with-brms",
    "href": "supplement_1.html#fit-a-statistical-model-with-brms",
    "title": "Supplement 1: Estimate expected potential outcomes in a hypothetical RCT",
    "section": "Fit a statistical model with brms",
    "text": "Fit a statistical model with brms\nWe now want to demonstrate how one could use the sample we just “collected” to estimate the expected potential outcomes we specified earlier. Potential Outcomes adn causal effects can be retrieved under the assumptions of consistency, exchangeability and positivity. For treatment regimes with interventions at both time points, we ensure exchangeability by leveraging the property that, given \\(L\\) and \\(A_1\\), \\(A_2\\) is independent of \\(U\\): \\(P(A_2 = a_2 | A_1, L, U) = P(A_2 = a_2 | A_1, L)\\).\nFor estimation, we use Bayesian regression models fit with the {brms} package to compute the parametric g-formula. The correctly specified regression model based on our assumed data generating process looks as follows:\n\nset.seed(42)\nbf_L &lt;- bf(L ~ A1)\nbf_Y &lt;- bf(Y ~ A1 + A2 + A1:A2 + L)\nfit &lt;- brm(bf_L + bf_Y + set_rescor(FALSE),\n  data = dat, chains = 4, cores = 4, backend = \"cmdstanr\", refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n\nsummary(fit)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: L ~ A1 \n         Y ~ A1 + A2 + A1:A2 + L \n   Data: dat (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nL_Intercept    -0.06      0.04    -0.15     0.03 1.00     6379     3045\nY_Intercept     9.94      0.07     9.81    10.07 1.00     3419     2984\nL_A1            1.04      0.06     0.92     1.17 1.00     6719     3107\nY_A1            0.95      0.09     0.78     1.12 1.00     2967     2975\nY_A2           -2.92      0.10    -3.11    -2.74 1.00     3263     3119\nY_L            -2.00      0.02    -2.05    -1.95 1.00     4845     3227\nY_A1:A2        -0.25      0.27    -0.78     0.29 1.00     5005     3428\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_L     1.42      0.02     1.38     1.47 1.00     7518     2878\nsigma_Y     1.41      0.02     1.37     1.45 1.00     7069     3101\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that the expected potential outcomes are not identical to the regression parameters. However, we can use these parameter to compute ubiased estimates."
  },
  {
    "objectID": "supplement_1.html#estimate-the-expected-potential-outcomes",
    "href": "supplement_1.html#estimate-the-expected-potential-outcomes",
    "title": "Supplement 1: Estimate expected potential outcomes in a hypothetical RCT",
    "section": "Estimate the expected potential outcomes",
    "text": "Estimate the expected potential outcomes\nWe first extract all parameter estimates from the model fit using the tidy_draws function from the {tidybayes} package. We use our assumptions of the data generating process to reproduce the expected potential outcomes. That means, we repeat the same simulation we already used to determine the true expected outcomes. But now, we do not use the true parameter values but instead the parameter estimates we obtained from fitting our statistical model to our single sample from the data generating process.\n\nparams &lt;- tidy_draws(fit)\nN &lt;- 10000\n\nE_Y_est &lt;- params |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_00 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_11 = {\n      a1 &lt;- rep(1, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(1, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_10 = {\n      a1 &lt;- rep(1, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_01 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(1, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)}) |&gt;\n  ungroup() |&gt; \n  select(E_Y_00, E_Y_11, E_Y_10, E_Y_01) |&gt;\n  summarise_draws()\n\nBecause we used Bayesian model estimation and extracted an approximation of the posterior distribution (4000 draws) for each model parameter, we not only get point estimates (posterior median) but can easily compute credibility intervals (symmetric .95 posterior intervals) for our estimates of the expected potential outcomes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Materials for Junker et al. (2024): Joint Effects in Psychology",
    "section": "",
    "text": "This is the companion website for the manuscript:\n\nJunker, L., Schoedel, R., & Pargent, F. (2024). Towards a Clearer Understanding of Causal Estimands: The Importance of Joint Effects in Longitudinal Designs with Time-Varying Treatments. https://doi.org/10.31234/osf.io/zmh5a\n\nYou can find the preprint on PsyArXiv here. You can find all code and materials here.\n\nThe website contains the supplementary materials:\n\nSupplement 1: Estimate expected potential outcomes in a hypothetical RCT\nSupplement 2: Estimate different causal effects in a hypothetical RCT\n\nNote: Supplement 1 computes Table 1 from the manuscript, while Supplement 2 extends our simulated example to various total, joint, and direct effects.\n\n\n\n Back to topReuseCC BY 4.0CitationBibTeX citation:@article{junker2024,\n  author = {Junker, Lukas and Schoedel, Ramona and Pargent, Florian},\n  title = {Towards a {Clearer} {Understanding} of {Causal} {Estimands:}\n    {The} {Importance} of {Joint} {Effects} in {Longitudinal} {Designs}\n    with {Time-Varying} {Treatments.}},\n  journal = {PsyArXiv},\n  date = {2024},\n  url = {https://FlorianPargent.github.io/joint_effects_ampps/},\n  doi = {10.31234/osf.io/zmh5a},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJunker, L., Schoedel, R., & Pargent, F. (2024). Towards a Clearer\nUnderstanding of Causal Estimands: The Importance of Joint Effects in\nLongitudinal Designs with Time-Varying Treatments. PsyArXiv. https://doi.org/10.31234/osf.io/zmh5a"
  },
  {
    "objectID": "supplement_2.html",
    "href": "supplement_2.html",
    "title": "Supplement 2: Estimate different causal effects in a hypothetical RCT",
    "section": "",
    "text": "In our manuscript, we introduce the following hypothetical RCT in the section Different Estimands in a Longitudinal Study:\nConsider a smartphone study in which participants are randomly assigned to use their smartphone from 8 PM to 10 PM (\\(t = 1\\)) or not. No intervention occurs afterward, which means that from 10 PM to midnight (\\(t = 2\\)), participants can decide for themselves whether to use their smartphone and this usage is passively recorded. We set \\(a_t=1\\) if there was any smartphone usage during time frame \\(t\\), while \\(a_t=0\\) means that the smartphone was not used at all during \\(t\\). At 10 PM, participants’ rumination (\\(L\\)) is also measured, as it is hypothesized to be another mediator between smartphone usage and sleep quality. Sleep quality (\\(Y\\)), the final outcome of the study, is measured the following morning. Figure 1 presents a DAG corresponding to this RCT.\nIn Supplement 1 we showed how to estimate expected potential outcomes and reproduce Table 1 in our manuscript. Here in Supplement 2, we will demonstrate in R how we can simulate and (try to) estimate different total, joint, and direct causal effects."
  },
  {
    "objectID": "supplement_2.html#load-packages",
    "href": "supplement_2.html#load-packages",
    "title": "Supplement 2: Estimate different causal effects in a hypothetical RCT",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(brms)\nlibrary(cmdstanr)\nlibrary(tidybayes)"
  },
  {
    "objectID": "supplement_2.html#specify-the-data-generating-process",
    "href": "supplement_2.html#specify-the-data-generating-process",
    "title": "Supplement 2: Estimate different causal effects in a hypothetical RCT",
    "section": "Specify the data generating process",
    "text": "Specify the data generating process\n\nDraw the DAG\n\n\nCode\ndag &lt;- dagitty('dag{\n  A1 -&gt; L &lt;- U\n  A1 -&gt; A2 &lt;- L\n  A1 -&gt; Y &lt;- A2\n  L -&gt; Y &lt;- U\n  A1[pos=\"0,2\"]\n  A2[pos=\"1,2\"]\n  Y[pos=\"1.5,1.5\"]\n  L[pos=\"0.5,1.5\"]\n  U[pos=\"1,1\"]\n  }')\nplot(dag)\n\n\n\n\n\n\n\n\nFigure 1: DAG that corresponds to an RCT with repeated measurements. \\(U\\) denotes unmeasured confounding, while \\(L\\) and \\(A_2\\) are measured mediators of the causal effect of \\(A_1\\) on the end of study outcome \\(Y\\). Since \\(A_1\\) is randomized, there are no arrows pointing into it.\n\n\n\n\n\n\n\nSpecify the functional relationships\nTo showcase joint effect estimation, we again simulate a scenario where \\(U\\) affects \\(A_2\\) only through measured \\(L\\). We discuss situations in which this assumption may be plausible in psychological research—induced by covariate-driven treatment assignment—in the latter half of the manuscript.\n\nb_L_A1 &lt;- 1\nb_L_U &lt;- 1\nb_A2_A1 &lt;- -3\nb_A2_L &lt;- 0.5\nb_A2_U &lt;- 0\nb_Y_A1 &lt;- -0.1\nb_Y_A2 &lt;- -3\nb_Y_L &lt;- -1\nb_Y_A1A2 &lt;- -0.5\nb_Y_U &lt;- -2\n\n\nf_U &lt;- function(){\n  rnorm(n)}\nf_A1 &lt;- function(){\n  rbinom(n, size = 1, prob = 0.5)}\nf_L &lt;- function(A1, U){\n  rnorm(n, mean = b_L_A1 * A1 + b_L_U * U)}\nf_A2 &lt;- function(A1, L){\n  rbinom(n, size = 1, prob = pnorm(b_A2_A1 * A1 + b_A2_L* L + b_A2_U * U))}\nf_Y &lt;- function(A1, A2, L, U){\n  rnorm(n, mean = 10 + b_Y_A1 * A1 + b_Y_A2 * A2 + b_Y_A1A2 * A1 * A2 + b_Y_L * L +\n      b_Y_U * U, sd = 0.1)}"
  },
  {
    "objectID": "supplement_2.html#determine-the-true-causal-effects",
    "href": "supplement_2.html#determine-the-true-causal-effects",
    "title": "Supplement 2: Estimate different causal effects in a hypothetical RCT",
    "section": "Determine the true causal effects",
    "text": "Determine the true causal effects\nWe will now simulate various causal effects for our hypothetical RCT:\n\nTotal effect of A1: \\(E(Y^{a_1=1}) - E(Y^{a_1=0})\\)\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_1: Y^{a_1=1}\n\nU &lt;- f_U()\nA1 &lt;- rep(1, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- f_A2(A1, L)\nY_1 &lt;- f_Y(A1, A2, L, U)\n\n# Y_0: Y^{a_1=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- f_A2(A1, L)\nY_0 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_1) - E(Y_0)\n(total_A1 &lt;- mean(Y_1) - mean(Y_0))\n\n[1] 0.3257348\n\n\n\n\nJoint effect “always - never”: \\(E(Y^{a_1=1, a_2=1}) - E(Y^{a_1=0, a_2=0})\\)\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_11: Y^{a_1=1, a_2=1}\n\nU &lt;- f_U()\nA1 &lt;- rep(1, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(1, n) # intervention\nY_11 &lt;- f_Y(A1, A2, L, U)\n\n# Y_00: Y^{a_1=0, a_2=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;-  rep(0, n) # intervention\nY_00 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_11) - E(Y_00)\n(joint_always &lt;- mean(Y_11) - mean(Y_00))\n\n[1] -4.600175\n\n\n\n\nJoint effect “early use”: \\(E(Y^{a_1=1, a_2=0}) - E(Y^{a_1=0, a_2=0})\\)\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_10: Y^{a_1=1, a_2=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(1, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(0, n) # intervention\nY_10 &lt;- f_Y(A1, A2, L, U)\n\n# Y_00: Y^{a_1=0, a_2=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(0, n) # intervention\nY_00 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_10) - E(Y_00)\n(joint_early &lt;- mean(Y_10) - mean(Y_00))\n\n[1] -1.100175\n\n\n\n\nJoint effect “late use”: \\(E(Y^{a_1=0, a_2=1}) - E(Y^{a_1=0, a_2=0})\\)\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_01: Y^{a_1=0, a_2=1}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(1, n) # intervention\nY_10 &lt;- f_Y(A1, A2, L, U)\n\n# Y_00: Y^{a_1=0, a_2=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(0, n) # intervention\nY_00 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_01) - E(Y_00)\n(joint_late &lt;- mean(Y_10) - mean(Y_00))\n\n[1] -3.000175\n\n\n\n\nDirect effect “always - never”: \\(E(Y^{a_1=1, L^{a_1=0}, a_2=1}) - E(Y^{a_1=0, L^{a_1=0}, a_2=0})\\)\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_10: Y^{a_1=1, L^{a_1=0}, a_2=1}\n\nU &lt;- f_U()\nA1 &lt;- rep(1, n) # intervention\nL &lt;- f_L(A1 = rep(0, n), U)\nA2 &lt;- rep(1, n) # intervention\nY_10 &lt;- f_Y(A1, A2, L, U)\n\n# Y_00: Y^{a_1=0, L^{a_1=0}, a_2=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;-  f_L(A1, U)\nA2 &lt;-  rep(0, n) # intervention\nY_00 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_10) - E(Y_00)\n(direct_always &lt;- mean(Y_10) - mean(Y_00))\n\n[1] -3.600175\n\n\n\n\nDirect effect “early use”: \\(E(Y^{a_1=1, L^{a_1=0}, a_2=0}) - E(Y^{a_1=0, L^{a_1=0}, a_2=0})\\)\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_10: Y^{a_1=1, L^{a_1=0}, a_2=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(1, n) # intervention\nL &lt;- f_L(A1 = rep(0, n), U)\nA2 &lt;- rep(0, n) # intervention\nY_10 &lt;- f_Y(A1, A2, L, U)\n\n# Y_00: Y^{a_1=0, L^{a_1=0}, a_2=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;-  f_L(A1, U)\nA2 &lt;-  rep(0, n) # intervention\nY_00 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_10) - E(Y_00)\n(direct_early &lt;- mean(Y_10) - mean(Y_00))\n\n[1] -0.1001748\n\n\n\n\nDirect effect “late use”: \\(E(Y^{a_1=0, L^{a_1=0}, a_2=0}) - E(Y^{a_1=0, L^{a_1=0}, a_2=1})\\)\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_01: Y^{a_1=0, L^{a_1=0}, a_2=1}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;- f_L(A1, U)\nA2 &lt;- rep(1, n) # intervention\nY_01 &lt;- f_Y(A1, A2, L, U)\n\n# Y_00: Y^{a_1=0, L^{a_1=0}, a_2=0}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;-  f_L(A1, U)\nA2 &lt;-  rep(0, n) # intervention\nY_00 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_01) - E(Y_00)\n(direct_late &lt;- mean(Y_01) - mean(Y_00))\n\n[1] -3.000175\n\n\n\n\nDirect effect of A1: \\(E(Y^{a_1=1, L^{a_1=0}, A_2^{a_1=0, L^{a_1=0}}}) - E(Y^{a_1=0, L^{a_1=0}, A_2^{a_1=0, L^{a_1=0}}})\\)\n\nn &lt;- 10000000\nset.seed(42)\n\n# Y_1: Y^{a_1=1, L^{a_1=0}, A_2^{a_1=0, L^{a_1=0}}}\n\nU &lt;- f_U()\nA1 &lt;- rep(1, n) # intervention\nL &lt;- f_L(A1 = rep(0, n), U)\nA2 &lt;- f_A2(A1 = rep(0, n), L)\nY_1 &lt;- f_Y(A1, A2, L, U)\n\n# Y_0: Y^{a_1=0, L^{a_1=0}, A_2^{a_1=0, L^{a_1=0}}}\n\nU &lt;- f_U()\nA1 &lt;- rep(0, n) # intervention\nL &lt;-  f_L(A1, U)\nA2 &lt;- f_A2(A1, L)\nY_0 &lt;- f_Y(A1, A2, L, U)\n\n# E(Y_1) - E(Y_0)\n(direct_A1 &lt;- mean(Y_1) - mean(Y_0))\n\n[1] -0.351657"
  },
  {
    "objectID": "supplement_2.html#simulate-a-sample-dataset",
    "href": "supplement_2.html#simulate-a-sample-dataset",
    "title": "Supplement 2: Estimate different causal effects in a hypothetical RCT",
    "section": "Simulate a sample dataset",
    "text": "Simulate a sample dataset\n\nn &lt;- 2000\nset.seed(42)\n\nU &lt;- f_U()\nA1 &lt;- f_A1()\nL &lt;- f_L(A1, U)\nA2 &lt;- f_A2(A1, L)\nY &lt;- f_Y(A1, A2, L, U)\n\ndat &lt;- data.frame(A1, A2, L, Y)"
  },
  {
    "objectID": "supplement_2.html#fit-statistical-models-with-brms",
    "href": "supplement_2.html#fit-statistical-models-with-brms",
    "title": "Supplement 2: Estimate different causal effects in a hypothetical RCT",
    "section": "Fit statistical models with brms",
    "text": "Fit statistical models with brms\nIn the hypothetical RCT, there was no randomization at \\(t = 2\\). However, we can still estimate causal effects using our knowledge of the DAG. Causal effects can be retrieved under the assumptions of consistency, exchangeability, and positivity.\nDifferent statistical models are required to estimate various total, joint, and direct effects. To estimate the joint effect, we ensure exchangeability by leveraging the property that, given \\(L\\) and \\(A_1\\), \\(A_2\\) is independent of \\(U\\): \\(P(A_2 = a_2 | A_1, L, U) = P(A_2 = a_2 | A_1, L)\\).\n\nset.seed(42)\nfit_total &lt;- brm(Y ~ A1,\n  data = dat, chains = 4, cores = 4, backend = \"cmdstanr\", refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.2 seconds.\n\nsummary(fit_total)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Y ~ A1 \n   Data: dat (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     8.62      0.12     8.38     8.85 1.00     3669     2699\nA1            0.22      0.16    -0.10     0.55 1.00     4111     2991\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.67      0.06     3.56     3.79 1.00     4181     3032\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nset.seed(42)\nbf_L &lt;- bf(L ~ A1)\nbf_Y &lt;- bf(Y ~ A1 + A2 + A1:A2 + L)\nfit_joint &lt;- brm(bf_L + bf_Y + set_rescor(FALSE),\n  data = dat, chains = 4, cores = 4, backend = \"cmdstanr\", refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.3 seconds.\n\nsummary(fit_joint)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: L ~ A1 \n         Y ~ A1 + A2 + A1:A2 + L \n   Data: dat (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nL_Intercept    -0.06      0.04    -0.15     0.03 1.00     6379     3045\nY_Intercept     9.94      0.07     9.81    10.07 1.00     3419     2984\nL_A1            1.04      0.06     0.92     1.17 1.00     6719     3107\nY_A1            0.95      0.09     0.78     1.12 1.00     2967     2975\nY_A2           -2.92      0.10    -3.11    -2.74 1.00     3263     3119\nY_L            -2.00      0.02    -2.05    -1.95 1.00     4845     3227\nY_A1:A2        -0.25      0.27    -0.78     0.29 1.00     5005     3428\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_L     1.42      0.02     1.38     1.47 1.00     7518     2878\nsigma_Y     1.41      0.02     1.37     1.45 1.00     7069     3101\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nset.seed(42)\nbf_L &lt;- bf(L ~ A1)\nbf_A2 &lt;- bf(A2 ~ A1 + L, family = bernoulli(link = probit))\nbf_Y &lt;- bf(Y ~ A1 + A2 + A1:A2 + L)\nfit_direct &lt;- brm(bf_L + bf_A2 + bf_Y + set_rescor(FALSE),\n  data = dat, chains = 4, cores = 4, backend = \"cmdstanr\", refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 finished in 2.3 seconds.\nChain 2 finished in 2.4 seconds.\nChain 4 finished in 2.4 seconds.\nChain 3 finished in 2.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.4 seconds.\nTotal execution time: 2.6 seconds.\n\nsummary(fit_direct)\n\n Family: MV(gaussian, bernoulli, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = probit\n         mu = identity; sigma = identity \nFormula: L ~ A1 \n         A2 ~ A1 + L \n         Y ~ A1 + A2 + A1:A2 + L \n   Data: dat (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nL_Intercept     -0.06      0.05    -0.15     0.03 1.00     5199     3225\nA2_Intercept     0.00      0.05    -0.09     0.09 1.00     5676     2987\nY_Intercept      9.94      0.07     9.82    10.07 1.00     2978     2936\nL_A1             1.04      0.06     0.92     1.17 1.00     5424     3241\nA2_A1           -2.83      0.13    -3.08    -2.58 1.00     2437     2966\nA2_L             0.50      0.03     0.44     0.57 1.00     3249     3052\nY_A1             0.95      0.09     0.77     1.12 1.00     2403     2728\nY_A2            -2.92      0.10    -3.11    -2.73 1.00     2937     2915\nY_L             -2.00      0.02    -2.05    -1.95 1.00     3521     3206\nY_A1:A2         -0.25      0.29    -0.84     0.31 1.00     4778     3432\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_L     1.42      0.02     1.38     1.47 1.00     4393     2804\nsigma_Y     1.41      0.02     1.36     1.46 1.00     5205     3063\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "supplement_2.html#estimate-the-causal-effects",
    "href": "supplement_2.html#estimate-the-causal-effects",
    "title": "Supplement 2: Estimate different causal effects in a hypothetical RCT",
    "section": "Estimate the causal effects",
    "text": "Estimate the causal effects\n\nparams_total &lt;- tidy_draws(fit_total)\nparams_joint &lt;- tidy_draws(fit_joint)\nparams_direct &lt;- tidy_draws(fit_direct)\nN &lt;- 10000\n\n# total effect A1\ntotal_A1_est &lt;- params_total |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_1 = {\n      a1 &lt;- rep(1, N)\n      y &lt;- rnorm(N, b_Intercept + b_A1 * a1, sd = sigma)\n      mean(y)},\n    E_Y_0 = {\n      a1 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Intercept + b_A1 * a1, sd = sigma)\n      mean(y)}) |&gt;\n  mutate(E_Y_diff = E_Y_1 - E_Y_0) |&gt;\n  ungroup() |&gt; select(E_Y_diff) |&gt;\n  summarise_draws()\n\n# joint effect always - never\njoint_always_est &lt;- params_joint |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_11 = {\n      a1 &lt;- rep(1, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(1, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_00 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)}) |&gt;\n  mutate(E_Y_diff = E_Y_11 - E_Y_00) |&gt;\n  ungroup() |&gt; select(E_Y_diff) |&gt;\n  summarise_draws()\n\n# joint effect early use\njoint_early_est &lt;- params_joint |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_10 = {\n      a1 &lt;- rep(1, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_00 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)}) |&gt;\n  mutate(E_Y_diff = E_Y_10 - E_Y_00) |&gt;\n  ungroup() |&gt; select(E_Y_diff) |&gt;\n  summarise_draws()\n\n# joint effect late use\njoint_late_est &lt;- params_joint |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_01 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(1, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_00 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)}) |&gt;\n  mutate(E_Y_diff = E_Y_01 - E_Y_00) |&gt;\n  ungroup() |&gt; select(E_Y_diff) |&gt;\n  summarise_draws()\n\n# direct effect always - never\ndirect_always_est &lt;- params_joint |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_11 = {\n      a1 &lt;- rep(1, N)\n      a1_0 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1_0, sd = sigma_L)\n      a2 &lt;- rep(1, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_00 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)}) |&gt;\n  mutate(E_Y_diff = E_Y_11 - E_Y_00) |&gt;\n  ungroup() |&gt; select(E_Y_diff) |&gt;\n  summarise_draws()\n\n# direct effect early use\ndirect_early_est &lt;- params_joint |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_10 = {\n      a1 &lt;- rep(1, N)\n      a1_0 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1_0, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_00 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)}) |&gt;\n  mutate(E_Y_diff = E_Y_10 - E_Y_00) |&gt;\n  ungroup() |&gt; select(E_Y_diff) |&gt;\n  summarise_draws()\n\n# direct effect late use\ndirect_late_est &lt;- params_joint |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_01 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(1, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_00 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rep(0, N)\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)}) |&gt;\n  mutate(E_Y_diff = E_Y_01 - E_Y_00) |&gt;\n  ungroup() |&gt; select(E_Y_diff) |&gt;\n  summarise_draws()\n\n# direct effect A1\ndirect_A1_est &lt;- params_direct |&gt; \n  group_by(.draw) |&gt;\n  mutate(\n    E_Y_1 = {\n      a1 &lt;- rep(1, N)\n      a1_0 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1_0, sd = sigma_L)\n      a2 &lt;- rbinom(N, size = 1, prob = pnorm(b_A2_Intercept + b_A2_A1 * a1_0 + b_A2_L * l))\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)},\n    E_Y_0 = {\n      a1 &lt;- rep(0, N)\n      l &lt;- rnorm(N, b_L_Intercept + b_L_A1 * a1, sd = sigma_L)\n      a2 &lt;- rbinom(N, size = 1, prob = pnorm(b_A2_Intercept + b_A2_A1 * a1 + b_A2_L * l))\n      y &lt;- rnorm(N, b_Y_Intercept + b_Y_A1 * a1 + b_Y_A2 * a2 + `b_Y_A1:A2` * a1 * a2 +\n          b_Y_L * l, sd = sigma_Y)\n      mean(y)}) |&gt;\n  mutate(E_Y_diff = E_Y_1 - E_Y_0) |&gt;\n  ungroup() |&gt; select(E_Y_diff) |&gt;\n  summarise_draws()"
  }
]